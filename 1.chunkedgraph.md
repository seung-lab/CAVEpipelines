<!-- TOC -->
* [Infrastructure](#infrastructure)
  * [Requirements](#requirements)
* [Terraform](#terraform-docs)
  * [Private Cluster](#private-cluster)
* [Docker Image](#docker-image)
* [Helm](#helm-docs)
  * [Chart](#chart-installation)
* [Ingest](#ingest)
* [Scaling](#scaling)
* [Troubleshooting](#troubleshooting)
  * [Retrying](#retrying)
  * [Failed Jobs](#failed-jobs)
<!-- TOC -->

## Infrastructure
 `terraform` is used to create the infrastructure needed to run ingest. Currently scripts are provided to run the ingest on Google Cloud, but it can be run locally or on another cloud provider with appropriate setup.

### Requirements
* GCloud SDK (tested with 378.0.0)
* Terraform (v1.1.7)
* Helm (v3.7.0)
* kubectl (v1.23.5)

## Terraform ([docs](https://www.terraform.io/docs))

> IMPORTANT: This setup assumes that a bigtable instance is already created. To reduce latency, it is recommended that all resources are co-located in the same region as bigtable instance.

Provided scripts create a VPC network, subnet, redis instance, cluster with separately managed pools to run master and workers. Customize variables in the file `terraform/terraform.tfvars` to create infrastructure in your Google Cloud project.

Create a service account with at least the following roles:
* Service Account User
* Cloud Memorystore Redis Admin
* Kubernetes Engine Cluster Admin
* Compute Network Admin

Put your service account json secret in this folder, name the file `google-secret.json`. This is used by `terraform` to access Google Cloud APIs.
Make sure the service account has the necessary permissions to create aforementioned resources. Refer `terraform/main.tf`.

Run the `terraform apply` command to create resources.

```shell
$ cd terraform/
$ terraform init // only needed first time
$ terraform apply
```
This will output some variables useful for next steps:
```
kubernetes_cluster_context = "gcloud container clusters get-credentials chunkedgraph-ingest --zone us-east1-b --project neuromancer-seung-import"
kubernetes_cluster_name = "chunkedgraph-ingest"
project_id = "neuromancer-seung-import"
redis_host = "10.128.211.211"
region = "us-east1"
zone = "us-east1-b"
```

Use value of `kubernetes_cluster_context` to connect to your cluster.
Use value of `redis_host` in `helm/pychunkedgraph/values.yaml` (more info in Helm section).

### Private Cluster
If your project does not have enough quota for In-Use IP addresses for the given region, you may need to use a private cluster. GCP assigns an external IP to each VM that is part of a public cluster which can easily hit the quota when you use a large number of workers.

To use a private cluster, add the following changes to `terraform/cluster.tf` within the `google_container_cluster` resource block.
```
  addons_config {
    http_load_balancing {
      disabled = true
    }
    network_policy_config { // Calico
      disabled = false
    }
  }

  network_policy {
    enabled = true
  }

  master_authorized_networks_config {}
  private_cluster_config {
    enable_private_nodes    = true
    enable_private_endpoint = false
    master_ipv4_cidr_block  = "172.16.0.16/28"
  }
```

After this is added, run terraform commands to create the required infrastructure. Once it completes we will need to [disable authorized networks](https://cloud.google.com/kubernetes-engine/docs/how-to/authorized-networks#disable) to allow `kubectl` access to kubernetes control plane.

```shell
gcloud container clusters update <CLUSTER_NAME> --no-enable-master-authorized-networks
```

This method allows us to use a private cluster easily.

If you need to give cluster access only to specific IPs you may follow the instructions in [this guide](https://github.com/GoogleCloudPlatform/gke-private-cluster-demo), this involves a few additional components and steps to access the cluster with `kubectl`.

## Docker image
Build the PyChunkedgraph docker image:
```shell
$ git clone https://github.com/seung-lab/PyChunkedGraph.git
$ cd PyChunkedGraph
$ gcloud builds submit . --tag=gcr.io/<your_project_id>/pychunkedgraph:<custom_tag> --project=<your_project_id>
```

and link the image and tag in `values.yaml` (refer to `example_values.yaml`).
```yaml
- name: *name
    image: &image
    repository: &imageRep gcr.io/<your_project_id>/pychunkedgraph
    tag: &tag <custom_tag>
```

## Helm ([docs](https://helm.sh/docs/))
`helm` is used to run the ingest. The provided chart installs kubernetes resources such as configmaps, secrets, deployments needed to run the ingest. Refer to example `helm/pychunkedgraph/example_values.yaml` file for more information.

> NOTE: Depending on your dataset, you will need to figure out the optimal limits for cpu and memory in your worker deployments. To do that adjust the `count` and `machine` variables in terraform.tfvars. It can vary with chunk size, size of supervoxels (atomic semgents in layer 1), number of edges per chunk and so on.

### Chart Installation
When all variables are ready, rename your values file to `values.yaml` (ignored by git because it can contain sensitive information). Then run:

```shell
$ cd helm/pychunkedgraph
$ helm dependencies update
$ helm install <release_name> . --debug --dry-run
$ helm upgrade <release_name> . --debug // to upgrade existing after changing values.yaml
```
If successful run the same command without `--dry-run`. This will create master and worker kubernetes deployments.

Pods will have dataset configuration mounted in `/app/datasets` and `/app` is the `WORKDIR`.

> NOTE: Refer to [dataset_config.md](dataset_config.md) for more information about the config structure.

> NOTE: The number of `workerDeployments` in `values.yaml` relates to the number of layers in the graph. For example, a graph with 6 layers must have 5 deployments in `workerDeployments` (`l2` - `l6`). To disable a depoyment, set the `enabled` key to `false`. Also, if you add or remove worker deployments, `trackerDepoyments` need to be changed accordingly. Note that the highest layer does not need a tracker because it has only one final root chunk.

## Ingest

The process is named `ingest` because we are ingesting data into a bigtable.

Pods should now be in `Running` status, provided there were no issues. Run the following to create a bigtable and enqueue jobs.

```shell
$ kubectl exec -ti deploy/master -- bash
// now you're in the container
> ingest graph <unique_test_bigtable_name> datasets/test.yml --test
```

[RQ](https://python-rq.org/docs/) is used to create jobs. This library uses `redis` as a task queue.

The `--test` flag will queue 8 children chunks that share the same parent. When the children chunk jobs finish, worker listening on the `t2` (tracker for layer 2) queue should enqueue the parent chunk. (To avoid race conditions there should only be one worker listening on tracker queues for each layer. The provided helm chart makes sure of this but important not to forget).

> NOTE: For large datasets, the `ingest graph` command can take a long time because queue is buffered so as not to exceed redis memory. You will need to figure out how big the redis instance must be accordingly. A 2 to 3GB instance seems sufficient for a dataset with 10M chunks at level 2, with a buffer length of 250000.

You can check the progress with `ingest status`. In another shell, run:
```shell
$ kubectl exec -ti deploy/master -- bash
> ingest status
2       : 8 / 64 # progress at each layer
3       : 1 / 8
4       : 0 / 1 # this graph has 4 layers
```
Output should look like this if successful. Now you can rerun the ingest without the `--test` flag (make sure to use a different bigtable name).

> NOTE: make sure to flush redis (`ingest flush_redis`) after running ingest and before another `helm install`. Residuals from previous ingest runs can lead to inaccurate information.

## Scaling
Coming soon.

## Troubleshooting

### Retrying
If the `ingest graph` command is killed by mistake or if kubernetes restarts `master` pod for any reason while ingest is in progress, you can restart the process with `--retry`. This option must only be used when a table has already been created and ingest process was interrupted.

```shell
> ingest graph <same_name_as_above> datasets/test.yml --retry
```

### Failed jobs
Sometimes jobs fail for any number of reasons. Assuming the causes were external, you can requeue them using `rqx requeue <queue_name> --all`. Refer to `pychunkedgraph/ingest/cli.py` and `pychunkedgraph/ingest/rq_cli.py` for more commands.

RQ Cheatsheet:
```shell
rq info # show info for all existing queues

rqx status <queue_name>
rqx failed <queue_name> # will print a list of failed jobs
rqx failed <queue_name> <job_id> # will print the traceback of error why the job failed
rqx requeue <queue_name> --all
rqx requeue <queue_name> <job_ids_space_separated>
```

More docs coming soon.
